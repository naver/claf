data_reader:
  dataset: "squad_bert"
  train_file_path: "data/bert/squad_synthetic_data.json"
  valid_file_path: "data/bert/squad_synthetic_data.json"
  squad_bert:
    lang_code: "en"
    max_seq_length: 384
    context_stride: 128
    max_question_length: 64

iterator:
  batch_size: 10

token:
  names:
    - "feature"
  types:
    - "feature"
  tokenizer:
    subword:
      name: "wordpiece"
      wordpiece:
        vocab_path: "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
    word:
      name: "bert_basic"
      bert_basic:
        do_lower_case: true

  feature:
    vocab:
      pretrained_path: "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      pretrained_token: "all"
    indexer:
      do_tokenize: false

model:
  name: "bert_for_qa"
  bert_for_qa:
    pretrained_model_name: "bert-base-uncased"

trainer:
  log_dir: "logs/test/bert_for_qa/"
  num_epochs: 2
  early_stopping_threshold: 2
  metric_key: "em"
  verbose_step_count: 1
  eval_and_save_step_count: 1

optimizer:
  learning_rate: 0.00005
  op_type: "adamw"
  adamw:
    weight_decay: 0.01
  lr_scheduler_type: "warmup_linear"
  warmup_linear:
    warmup_steps: 10000
  gradient_accumulation_steps: 2

seed_num: 25
